# recon/exceptions_bridge.py
"""
ì˜ˆì™¸â†’SKU ë¸Œë¦¬ì§€ ì˜êµ¬í™”
ì¸ë³´ì´ìŠ¤ ì—”ì§„ì´ ë§Œë“  Exceptions_Onlyë¥¼ SKU í‚¤ì— ì–¹ì–´ exceptions_by_sku í…Œì´ë¸”Â·íŒŒì¼€ ì €ì¥
"""
import sys
import os
from pathlib import Path
import pandas as pd
import duckdb
from typing import Optional, Dict, Any

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def exceptions_to_sku(
    invoice_dashboard_xlsx: str, 
    all_master_parquet: str, 
    out_dir: str = "out"
) -> pd.DataFrame:
    """
    Invoice Exceptionsë¥¼ SKU ë‹¨ìœ„ë¡œ ë§¤í•‘í•˜ì—¬ ì˜êµ¬ í…Œì´ë¸” ìƒì„±
    
    Args:
        invoice_dashboard_xlsx: Invoice ëŒ€ì‹œë³´ë“œ Excel íŒŒì¼
        all_master_parquet: SKU Master Parquet íŒŒì¼ (HVDC CODEâ†’SKU ë§¤í•‘ìš©)
        out_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    
    Returns:
        pd.DataFrame: SKU ë§¤í•‘ëœ ì˜ˆì™¸ ë°ì´í„°
    """
    print(f"[INFO] Processing exceptions to SKU bridge...")
    print(f"[INFO] Invoice dashboard: {invoice_dashboard_xlsx}")
    print(f"[INFO] SKU master: {all_master_parquet}")
    
    try:
        # 1) Invoice ëŒ€ì‹œë³´ë“œì—ì„œ Exceptions_Only ì½ê¸°
        xl = pd.ExcelFile(invoice_dashboard_xlsx)
        print(f"[INFO] Available sheets: {xl.sheet_names}")
        
        ex = pd.DataFrame()
        if "Exceptions_Only" in xl.sheet_names:
            ex = xl.parse("Exceptions_Only")
            print(f"[INFO] Exceptions_Only loaded: {ex.shape}")
        else:
            print("[WARN] Exceptions_Only sheet not found")
            return pd.DataFrame()
        
        # 2) SKU Masterì—ì„œ HVDC CODEâ†’SKU ë§¤í•‘ ì½ê¸°
        if not Path(all_master_parquet).exists():
            print(f"[ERROR] SKU Master parquet not found: {all_master_parquet}")
            return pd.DataFrame()
        
        # DuckDBë¥¼ í†µí•´ SKU Master ì½ê¸°
        con = duckdb.connect()
        sku_mapping = con.execute(f"SELECT SKU, hvdc_code_norm FROM read_parquet('{all_master_parquet}')").df()
        con.close()
        
        print(f"[INFO] SKU mapping loaded: {sku_mapping.shape}")
        
        # 3) Invoice RAW CODE ì •ê·œí™” ë° ë§¤í•‘
        if "Invoice_RAW_CODE" in ex.columns:
            ex["hvdc_code_norm"] = ex["Invoice_RAW_CODE"].astype(str).str.upper().str.replace(" ", "")
        elif "HVDC_CODE" in ex.columns:
            ex["hvdc_code_norm"] = ex["HVDC_CODE"].astype(str).str.upper().str.replace(" ", "")
        else:
            print("[WARN] No HVDC code column found in exceptions")
            ex["hvdc_code_norm"] = None
        
        # 4) SKU ë§¤í•‘
        out = ex.merge(sku_mapping, on="hvdc_code_norm", how="left")
        
        # 5) ë§¤í•‘ ê²°ê³¼ í†µê³„
        mapped_count = out["SKU"].notna().sum()
        total_count = len(out)
        mapping_rate = mapped_count / total_count if total_count > 0 else 0
        
        print(f"[INFO] SKU mapping results:")
        print(f"  - Total exceptions: {total_count}")
        print(f"  - Mapped to SKU: {mapped_count}")
        print(f"  - Mapping rate: {mapping_rate:.1%}")
        
        # 6) ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ë° ì €ì¥
        Path(out_dir).mkdir(exist_ok=True)
        
        # Parquet ì €ì¥
        output_file = f"{out_dir}/exceptions_by_sku.parquet"
        out.to_parquet(output_file, index=False)
        print(f"[INFO] Parquet saved: {output_file}")
        
        # DuckDB ì €ì¥
        db_file = f"{out_dir}/exceptions_by_sku.duckdb"
        con = duckdb.connect(db_file)
        con.execute("DROP TABLE IF EXISTS exceptions_by_sku")
        con.execute(f"CREATE TABLE exceptions_by_sku AS SELECT * FROM read_parquet('{output_file}')")
        
        # ì¸ë±ìŠ¤ ìƒì„±
        con.execute("CREATE INDEX IF NOT EXISTS idx_exceptions_sku ON exceptions_by_sku(SKU)")
        con.execute("CREATE INDEX IF NOT EXISTS idx_exceptions_hvdc ON exceptions_by_sku(hvdc_code_norm)")
        
        # ìš”ì•½ ë·° ìƒì„±
        con.execute("""
            CREATE OR REPLACE VIEW v_exceptions_summary AS
            SELECT 
                CASE 
                    WHEN SKU IS NOT NULL THEN 'Mapped'
                    ELSE 'Unmapped'
                END AS mapping_status,
                COUNT(*) AS count,
                COUNT(*) * 100.0 / SUM(COUNT(*)) OVER() AS percentage
            FROM exceptions_by_sku
            GROUP BY 1
        """)
        
        con.execute("""
            CREATE OR REPLACE VIEW v_exceptions_by_sku AS
            SELECT 
                SKU,
                hvdc_code_norm,
                COUNT(*) AS exception_count,
                STRING_AGG(DISTINCT Invoice_RAW_CODE, ', ') AS raw_codes,
                MIN(CASE WHEN Match_Status IS NOT NULL THEN Match_Status END) AS match_status
            FROM exceptions_by_sku
            WHERE SKU IS NOT NULL
            GROUP BY SKU, hvdc_code_norm
            ORDER BY exception_count DESC
        """)
        
        con.close()
        print(f"[INFO] DuckDB saved: {db_file}")
        
        return out
        
    except Exception as e:
        print(f"[ERROR] Failed to process exceptions to SKU: {e}")
        return pd.DataFrame()

def analyze_exceptions_patterns(exceptions_df: pd.DataFrame) -> Dict[str, Any]:
    """
    ì˜ˆì™¸ íŒ¨í„´ ë¶„ì„
    
    Args:
        exceptions_df: ì˜ˆì™¸ ë°ì´í„° DataFrame
    
    Returns:
        dict: ë¶„ì„ ê²°ê³¼
    """
    if exceptions_df.empty:
        return {"error": "Empty exceptions data"}
    
    analysis = {
        "total_exceptions": len(exceptions_df),
        "mapping_status": {},
        "top_error_patterns": {},
        "sku_coverage": {}
    }
    
    # ë§¤í•‘ ìƒíƒœ ë¶„ì„
    if "SKU" in exceptions_df.columns:
        mapped = exceptions_df["SKU"].notna().sum()
        unmapped = exceptions_df["SKU"].isna().sum()
        analysis["mapping_status"] = {
            "mapped": int(mapped),
            "unmapped": int(unmapped),
            "mapping_rate": mapped / len(exceptions_df)
        }
    
    # ìƒìœ„ ì˜¤ë¥˜ íŒ¨í„´
    if "Match_Status" in exceptions_df.columns:
        error_patterns = exceptions_df["Match_Status"].value_counts().head(10)
        analysis["top_error_patterns"] = error_patterns.to_dict()
    
    # SKUë³„ ì˜ˆì™¸ ë¶„í¬
    if "SKU" in exceptions_df.columns:
        sku_exceptions = exceptions_df["SKU"].value_counts()
        analysis["sku_coverage"] = {
            "unique_skus_with_exceptions": sku_exceptions.nunique(),
            "max_exceptions_per_sku": int(sku_exceptions.max()) if not sku_exceptions.empty else 0,
            "avg_exceptions_per_sku": float(sku_exceptions.mean()) if not sku_exceptions.empty else 0
        }
    
    return analysis

def create_exceptions_report(exceptions_df: pd.DataFrame, output_path: str = "out/exceptions_report.xlsx"):
    """
    ì˜ˆì™¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    
    Args:
        exceptions_df: ì˜ˆì™¸ ë°ì´í„° DataFrame
        output_path: ë¦¬í¬íŠ¸ ì¶œë ¥ ê²½ë¡œ
    """
    print(f"[INFO] Creating exceptions report: {output_path}")
    
    try:
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # ì›ë³¸ ë°ì´í„°
            exceptions_df.to_excel(writer, sheet_name='Exceptions_Data', index=False)
            
            # ë§¤í•‘ ìƒíƒœ ìš”ì•½
            if "SKU" in exceptions_df.columns:
                mapping_summary = pd.DataFrame({
                    'Status': ['Mapped', 'Unmapped'],
                    'Count': [
                        exceptions_df["SKU"].notna().sum(),
                        exceptions_df["SKU"].isna().sum()
                    ]
                })
                mapping_summary.to_excel(writer, sheet_name='Mapping_Summary', index=False)
            
            # SKUë³„ ì˜ˆì™¸ ë¶„í¬
            if "SKU" in exceptions_df.columns and exceptions_df["SKU"].notna().any():
                sku_distribution = exceptions_df["SKU"].value_counts().head(20).reset_index()
                sku_distribution.columns = ['SKU', 'Exception_Count']
                sku_distribution.to_excel(writer, sheet_name='SKU_Distribution', index=False)
            
            # ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
            if "Match_Status" in exceptions_df.columns:
                error_patterns = exceptions_df["Match_Status"].value_counts().reset_index()
                error_patterns.columns = ['Match_Status', 'Count']
                error_patterns.to_excel(writer, sheet_name='Error_Patterns', index=False)
        
        print(f"[SUCCESS] Exceptions report created: {output_path}")
        
    except Exception as e:
        print(f"[ERROR] Failed to create exceptions report: {e}")

def run_exceptions_bridge_pipeline(
    invoice_dashboard_xlsx: str,
    sku_master_parquet: str,
    out_dir: str = "out"
) -> Dict[str, Any]:
    """
    ì˜ˆì™¸â†’SKU ë¸Œë¦¬ì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    Args:
        invoice_dashboard_xlsx: Invoice ëŒ€ì‹œë³´ë“œ Excel íŒŒì¼
        sku_master_parquet: SKU Master Parquet íŒŒì¼
        out_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    
    Returns:
        dict: ì‹¤í–‰ ê²°ê³¼
    """
    print("=" * 50)
    print("ğŸ”— Exceptions â†’ SKU Bridge Pipeline")
    print("=" * 50)
    
    # 1) ì˜ˆì™¸â†’SKU ë§¤í•‘ ì‹¤í–‰
    exceptions_df = exceptions_to_sku(invoice_dashboard_xlsx, sku_master_parquet, out_dir)
    
    if exceptions_df.empty:
        return {"success": False, "error": "No exceptions data processed"}
    
    # 2) íŒ¨í„´ ë¶„ì„
    analysis = analyze_exceptions_patterns(exceptions_df)
    
    # 3) ë¦¬í¬íŠ¸ ìƒì„±
    report_path = f"{out_dir}/exceptions_report.xlsx"
    create_exceptions_report(exceptions_df, report_path)
    
    # 4) ê²°ê³¼ ìš”ì•½
    result = {
        "success": True,
        "total_exceptions": len(exceptions_df),
        "mapping_rate": analysis.get("mapping_status", {}).get("mapping_rate", 0),
        "unique_skus_affected": analysis.get("sku_coverage", {}).get("unique_skus_with_exceptions", 0),
        "output_files": {
            "parquet": f"{out_dir}/exceptions_by_sku.parquet",
            "duckdb": f"{out_dir}/exceptions_by_sku.duckdb",
            "report": report_path
        },
        "analysis": analysis
    }
    
    print("\n" + "=" * 50)
    print("âœ… Exceptions Bridge Pipeline ì™„ë£Œ!")
    print("=" * 50)
    print(f"ğŸ“Š ì´ ì˜ˆì™¸: {result['total_exceptions']}")
    print(f"ğŸ”— ë§¤í•‘ìœ¨: {result['mapping_rate']:.1%}")
    print(f"ğŸ·ï¸ ì˜í–¥ë°›ì€ SKU: {result['unique_skus_affected']}")
    print(f"ğŸ“ ì¶œë ¥ íŒŒì¼:")
    for file_type, path in result["output_files"].items():
        print(f"  - {file_type}: {path}")
    
    return result

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("Exceptions Bridge - í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
    invoice_file = "HVDC_Invoice_Validation_Dashboard.xlsx"
    sku_master_file = "out/SKU_MASTER_v2.parquet"
    
    if Path(invoice_file).exists() and Path(sku_master_file).exists():
        result = run_exceptions_bridge_pipeline(invoice_file, sku_master_file)
        print(f"\nê²°ê³¼: {result}")
    else:
        print(f"âŒ íŒŒì¼ ì—†ìŒ - Invoice: {Path(invoice_file).exists()}, SKU Master: {Path(sku_master_file).exists()}")
